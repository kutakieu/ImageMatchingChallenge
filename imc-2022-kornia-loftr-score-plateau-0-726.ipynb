{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"credit :[this notebook by ammarali32](https://www.kaggle.com/code/ammarali32/imc-2022-kornia-loftr-from-0-533-to-0-721) and followers...\n\n\n### In this experiment, with our dataset, the pretrained  LoFTR outdoors model seems to reach a plateau at 0.726.","metadata":{}},{"cell_type":"code","source":"!ls ..","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:53:39.248109Z","iopub.execute_input":"2022-05-08T07:53:39.248652Z","iopub.status.idle":"2022-05-08T07:53:40.008971Z","shell.execute_reply.started":"2022-05-08T07:53:39.248613Z","shell.execute_reply":"2022-05-08T07:53:40.007453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Install Libs***","metadata":{}},{"cell_type":"code","source":"%%capture\n#dry_run = False\n!pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n!pip install ../input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:43:32.928348Z","iopub.execute_input":"2022-05-07T10:43:32.92889Z","iopub.status.idle":"2022-05-07T10:44:29.545668Z","shell.execute_reply.started":"2022-05-07T10:43:32.92881Z","shell.execute_reply":"2022-05-07T10:44:29.544734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Import dependencies***","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport csv\nfrom glob import glob\nimport torch\nimport matplotlib.pyplot as plt\nimport kornia\nfrom kornia_moons.feature import *\nimport kornia as K\nimport kornia.feature as KF\nimport gc\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:45:37.314551Z","iopub.execute_input":"2022-05-07T10:45:37.314882Z","iopub.status.idle":"2022-05-07T10:45:37.321503Z","shell.execute_reply.started":"2022-05-07T10:45:37.314845Z","shell.execute_reply":"2022-05-07T10:45:37.320563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Model***","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda')\nmatcher = KF.LoFTR(pretrained=None)\nmatcher.load_state_dict(torch.load(\"../input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict'])\nmatcher = matcher.to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:44:31.713702Z","iopub.execute_input":"2022-05-07T10:44:31.713926Z","iopub.status.idle":"2022-05-07T10:44:35.817454Z","shell.execute_reply.started":"2022-05-07T10:44:31.713895Z","shell.execute_reply":"2022-05-07T10:44:35.816609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ***Utils***","metadata":{}},{"cell_type":"code","source":"src = '/kaggle/input/image-matching-challenge-2022/'\n\ntest_samples = []\nwith open(f'{src}/test.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        test_samples += [row]\n\n\ndef FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    \n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n\n\ndef load_torch_image(fname, device):\n    img = cv2.imread(fname)\n    scale = 840 / max(img.shape[0], img.shape[1]) \n    w = int(img.shape[1] * scale)\n    h = int(img.shape[0] * scale)\n    img = cv2.resize(img, (w, h))\n    img = K.image_to_tensor(img, False).float() /255.\n    img = K.color.bgr_to_rgb(img)\n    return img.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:44:35.820067Z","iopub.execute_input":"2022-05-07T10:44:35.820522Z","iopub.status.idle":"2022-05-07T10:44:35.832926Z","shell.execute_reply.started":"2022-05-07T10:44:35.820481Z","shell.execute_reply":"2022-05-07T10:44:35.832165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Parametric Study***\n\n|Base|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |||\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|**0.723** |0.726|0.726|0.726|0.726|0.726|0.726|0.726|0.726|0.726|0.725|0.725|0.725|0.725|0.725|0.725|0.723|0.723|0.723|0.723|0.722|0.697|0.653|0.608|\n|**0.25**|0.19|0.18|0.17|0.195|0.19|0.185|0.18|0.175|0.17|0.18|0.15|0.15|0.018|0.015|0.01|0.2|0.2|0.1|0.05|0.5|1|2|3|\n|**0.9999**|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|0.9999|\n|**100000**|300000|300000|300000|250000|250000|250000|250000|250000|250000|200000|300000|250000|200000|200000|200000|150000|250000|150000|150000|200000|200000|200000|200000|\n","metadata":{}},{"cell_type":"code","source":"x = [0.18,0.19,0.17, 0.195, 0.19, 0.185, 0.18, 0.175, 0.17, 0.18, 0.15, 0.15, 0.018, 0.015, 0.01, 0.2, 0.2, 0.1, 0.05, 0.5, 1, 2, 3 ]\ny = [30, 30,30, 25, 25, 25, 25, 25, 25, 20, 30, 25, 20,20, 20, 15, 25, 15, 15, 20, 20, 20, 20]\nz = [0.726, 0.726,  0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.723, 0.723, 0.723, 0.723, 0.722, 0.697, 0.653, 0.608]\n\n# Creating figure\nfig = plt.figure(figsize = (16, 9))\nax = plt.axes(projection =\"3d\")\ncolor_map = plt.get_cmap('cool')\nscatter_plot = ax.scatter3D(x, y, z,\n                            c=z, s=80,\n                            cmap = color_map)\n\n\nax.scatter(x, y, z, marker='o', cmap = color_map)\n# Creating Colorbar \nplt.colorbar(scatter_plot, shrink=0.55)\n\nax.set_xlabel('X Threshold ')\nax.set_ylabel('Y maxIters *10K')\nax.set_zlabel('z Score')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:44:35.835976Z","iopub.execute_input":"2022-05-07T10:44:35.836494Z","iopub.status.idle":"2022-05-07T10:44:36.179511Z","shell.execute_reply.started":"2022-05-07T10:44:35.836439Z","shell.execute_reply":"2022-05-07T10:44:36.178819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Takeaway\n\n- In this experiment, with our dataset, the pretrained LoFTR outdoors model seems to reach a plateau at 0.726.\n- OpenCV indicate that the treshold RANSAC Parameter should be set to 1-3. In our case, going significantly lower allows for a better scores.\n- The 0.726 scoring plateau seems to be around maxIters above 200000 and Treshold below 0.20 with a confidence parameter at 0.9999.","metadata":{}},{"cell_type":"markdown","source":"# ***Inference***","metadata":{}},{"cell_type":"code","source":"F_dict = {}\nimport time\nfor i, row in enumerate(test_samples):\n    sample_id, batch_id, image_1_id, image_2_id = row\n    # Load the images.\n    st = time.time()\n    image_1 = load_torch_image(f'{src}/test_images/{batch_id}/{image_1_id}.png', device)\n    image_2 = load_torch_image(f'{src}/test_images/{batch_id}/{image_2_id}.png', device)\n    print(image_1.shape)\n    input_dict = {\"image0\": K.color.rgb_to_grayscale(image_1), \n              \"image1\": K.color.rgb_to_grayscale(image_2)}\n\n    with torch.no_grad():\n        correspondences = matcher(input_dict)\n        \n    mkpts0 = correspondences['keypoints0'].cpu().numpy()\n    mkpts1 = correspondences['keypoints1'].cpu().numpy()\n    \n    if len(mkpts0) > 7:\n        F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.200, 0.9999, 250000)\n        inliers = inliers > 0\n        assert F.shape == (3, 3), 'Malformed F?'\n        F_dict[sample_id] = F\n    else:\n        F_dict[sample_id] = np.zeros((3, 3))\n        continue\n    gc.collect()\n    nd = time.time()    \n    if (i < 3):\n        print(\"Running time: \", nd - st, \" s\")\n        draw_LAF_matches(\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0).view(1,-1, 2),\n                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1)),\n\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1).view(1,-1, 2),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1)),\n        torch.arange(mkpts0.shape[0]).view(-1,1).repeat(1,2),\n        K.tensor_to_image(image_1),\n        K.tensor_to_image(image_2),\n        inliers,\n        draw_dict={'inlier_color': (0.2, 1, 0.2),\n                   'tentative_color': None, \n                   'feature_color': (0.2, 0.5, 1), 'vertical': False})\n    \nwith open('submission.csv', 'w') as f:\n    f.write('sample_id,fundamental_matrix\\n')\n    for sample_id, F in F_dict.items():\n        f.write(f'{sample_id},{FlattenMatrix(F)}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:45:44.233888Z","iopub.execute_input":"2022-05-07T10:45:44.234154Z","iopub.status.idle":"2022-05-07T10:45:53.616402Z","shell.execute_reply.started":"2022-05-07T10:45:44.234124Z","shell.execute_reply":"2022-05-07T10:45:53.615642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center>\n    <h2 style=\"color: #022047\"> Thanks for reading ðŸ¤—  </h2>\n</center>","metadata":{}}]}