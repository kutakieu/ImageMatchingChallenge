{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0e4843",
   "metadata": {
    "papermill": {
     "duration": 0.04296,
     "end_time": "2022-05-17T15:39:52.915051",
     "exception": false,
     "start_time": "2022-05-17T15:39:52.872091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ***Install Libs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678dc109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:39:53.009119Z",
     "iopub.status.busy": "2022-05-17T15:39:53.008108Z",
     "iopub.status.idle": "2022-05-17T15:40:46.873972Z",
     "shell.execute_reply": "2022-05-17T15:40:46.873134Z",
     "shell.execute_reply.started": "2022-05-17T15:36:58.224358Z"
    },
    "papermill": {
     "duration": 53.916344,
     "end_time": "2022-05-17T15:40:46.874187",
     "exception": false,
     "start_time": "2022-05-17T15:39:52.957843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#dry_run = False\n",
    "!pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n",
    "!pip install ../input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl\n",
    "\n",
    "# for depth estimation module\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp -r ../input/midasdepthestimation/MiDaS-master  /root/.cache/torch/hub/intel-isl_MiDaS_master\n",
    "!cp ../input/midasdepthestimation/dpt_large-midas-2f21e586.pt  /root/.cache/torch/hub/checkpoints/\n",
    "!pip install ../input/midasdepthestimation/timm-0.5.4-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d51b06",
   "metadata": {
    "papermill": {
     "duration": 0.028598,
     "end_time": "2022-05-17T15:40:46.928584",
     "exception": false,
     "start_time": "2022-05-17T15:40:46.899986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ***Import dependencies***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a34837e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:40:46.989989Z",
     "iopub.status.busy": "2022-05-17T15:40:46.989192Z",
     "iopub.status.idle": "2022-05-17T15:40:51.047519Z",
     "shell.execute_reply": "2022-05-17T15:40:51.048177Z",
     "shell.execute_reply.started": "2022-05-17T15:37:28.412264Z"
    },
    "papermill": {
     "duration": 4.092798,
     "end_time": "2022-05-17T15:40:51.048359",
     "exception": false,
     "start_time": "2022-05-17T15:40:46.955561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "from glob import glob\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# for LoFTR\n",
    "import kornia\n",
    "from kornia_moons.feature import *\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "# for depth-estimation\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e10cb69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:40:51.106643Z",
     "iopub.status.busy": "2022-05-17T15:40:51.105955Z",
     "iopub.status.idle": "2022-05-17T15:40:51.179126Z",
     "shell.execute_reply": "2022-05-17T15:40:51.179713Z",
     "shell.execute_reply.started": "2022-05-17T15:37:29.796187Z"
    },
    "papermill": {
     "duration": 0.106475,
     "end_time": "2022-05-17T15:40:51.179893",
     "exception": false,
     "start_time": "2022-05-17T15:40:51.073418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"../input/imcutils\")\n",
    "from imc_metric import EvaluateSubmission, ReadCovisibilityData, FlattenMatrix, LoadCalibration\n",
    "\n",
    "sys.path.append(\"../input/super-glue-pretrained-network\")\n",
    "from models.matching import Matching as SuperGlue\n",
    "from models.utils import (compute_pose_error, compute_epipolar_error,\n",
    "                          estimate_pose, make_matching_plot,\n",
    "                          error_colormap, AverageTimer, pose_auc, read_image,\n",
    "                          rotate_intrinsics, rotate_pose_inplane,\n",
    "                          scale_intrinsics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "379d195c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:40:51.237523Z",
     "iopub.status.busy": "2022-05-17T15:40:51.236821Z",
     "iopub.status.idle": "2022-05-17T15:40:51.270772Z",
     "shell.execute_reply": "2022-05-17T15:40:51.270163Z",
     "shell.execute_reply.started": "2022-05-17T15:37:29.831225Z"
    },
    "papermill": {
     "duration": 0.065071,
     "end_time": "2022-05-17T15:40:51.270928",
     "exception": false,
     "start_time": "2022-05-17T15:40:51.205857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for pytorch3d\n",
    "sys.path.append(\"../input/pytorch3ddependencies/pytorch3d_dependencies\")\n",
    "os.environ[\"CUB_HOME\"] = \"../input/pytorch3ddependencies/pytorch3d_dependencies/cub-1.10.0\"\n",
    "import pytorch3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf65e3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:40:51.328369Z",
     "iopub.status.busy": "2022-05-17T15:40:51.327653Z",
     "iopub.status.idle": "2022-05-17T15:40:52.149684Z",
     "shell.execute_reply": "2022-05-17T15:40:52.150445Z",
     "shell.execute_reply.started": "2022-05-17T15:37:29.842495Z"
    },
    "papermill": {
     "duration": 0.85359,
     "end_time": "2022-05-17T15:40:52.150733",
     "exception": false,
     "start_time": "2022-05-17T15:40:51.297143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch3d.renderer.cameras import (\n",
    "    PerspectiveCameras,\n",
    ")\n",
    "from pytorch3d.transforms.so3 import (\n",
    "    so3_exp_map\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba2cab6",
   "metadata": {
    "papermill": {
     "duration": 0.026231,
     "end_time": "2022-05-17T15:40:52.202566",
     "exception": false,
     "start_time": "2022-05-17T15:40:52.176335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ***Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bde12cfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:40:52.323333Z",
     "iopub.status.busy": "2022-05-17T15:40:52.319757Z",
     "iopub.status.idle": "2022-05-17T15:40:57.335834Z",
     "shell.execute_reply": "2022-05-17T15:40:57.336335Z",
     "shell.execute_reply.started": "2022-05-17T15:37:30.037110Z"
    },
    "papermill": {
     "duration": 5.108284,
     "end_time": "2022-05-17T15:40:57.336519",
     "exception": false,
     "start_time": "2022-05-17T15:40:52.228235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# LoFTR\n",
    "matcher = KF.LoFTR(pretrained=None)\n",
    "matcher.load_state_dict(torch.load(\"../input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict'])\n",
    "matcher = matcher.to(device)\n",
    "matcher.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce394afe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:40:57.397002Z",
     "iopub.status.busy": "2022-05-17T15:40:57.395970Z",
     "iopub.status.idle": "2022-05-17T15:40:58.251313Z",
     "shell.execute_reply": "2022-05-17T15:40:58.252208Z",
     "shell.execute_reply.started": "2022-05-17T15:37:32.087982Z"
    },
    "papermill": {
     "duration": 0.890297,
     "end_time": "2022-05-17T15:40:58.252509",
     "exception": false,
     "start_time": "2022-05-17T15:40:57.362212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SuperPoint model\n",
      "Loaded SuperGlue model (\"outdoor\" weights)\n"
     ]
    }
   ],
   "source": [
    "# Super Glue\n",
    "config = {\n",
    "    \"superpoint\": {\n",
    "        \"nms_radius\": 4,\n",
    "        \"keypoint_threshold\": 0.005,\n",
    "        \"max_keypoints\": 1024\n",
    "    },\n",
    "    \"superglue\": {\n",
    "        \"weights\": \"outdoor\",\n",
    "        \"sinkhorn_iterations\": 20,\n",
    "        \"match_threshold\": 0.2,\n",
    "    }\n",
    "}\n",
    "superglue = SuperGlue(config).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a05fc498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:40:58.320143Z",
     "iopub.status.busy": "2022-05-17T15:40:58.314550Z",
     "iopub.status.idle": "2022-05-17T15:40:58.325122Z",
     "shell.execute_reply": "2022-05-17T15:40:58.325695Z",
     "shell.execute_reply.started": "2022-05-17T15:37:32.298455Z"
    },
    "papermill": {
     "duration": 0.045682,
     "end_time": "2022-05-17T15:40:58.325881",
     "exception": false,
     "start_time": "2022-05-17T15:40:58.280199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def match(img_path0, img_path1, matcher, device=device):\n",
    "    img0 = load_torch_image(img_path0)\n",
    "    img1 = load_torch_image(img_path1)\n",
    "        \n",
    "    input_dict = {\"image0\": K.color.rgb_to_grayscale(img0).to(device), \n",
    "                  \"image1\": K.color.rgb_to_grayscale(img1).to(device)}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correspondences = matcher(input_dict)\n",
    "        \n",
    "    mkpts0 = correspondences['keypoints0'].cpu().numpy()\n",
    "    mkpts1 = correspondences['keypoints1'].cpu().numpy()\n",
    "        \n",
    "    return mkpts0, mkpts1\n",
    "\n",
    "def superglue_match(img_path0, img_path1, matcher, device=device):\n",
    "    resize = [-1, ]\n",
    "    resize_float = True\n",
    "    image_1, inp_1, scales_1 = read_image(img_path0, device, resize, 0, resize_float)\n",
    "    image_2, inp_2, scales_2 = read_image(img_path1, device, resize, 0, resize_float)\n",
    "    \n",
    "    pred = matcher({\"image0\": inp_1, \"image1\": inp_2})\n",
    "    pred = {k: v[0].detach().cpu().numpy() for k, v in pred.items()}\n",
    "    kpts1, kpts2 = pred[\"keypoints0\"], pred[\"keypoints1\"]\n",
    "    matches, conf = pred[\"matches0\"], pred[\"matching_scores0\"]\n",
    "    \n",
    "    valid = matches > -1\n",
    "    mkpts1 = kpts1[valid]\n",
    "    mkpts2 = kpts2[matches[valid]]\n",
    "    \n",
    "    return mkpts1, mkpts2\n",
    "\n",
    "def get_F_matrix(mkpts0, mkpts1):\n",
    "    # Make sure we do not trigger an exception here.\n",
    "    if len(mkpts0) > 8:\n",
    "        F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.200, 0.9999, 250000)\n",
    "        assert F.shape == (3, 3), 'Malformed F?'\n",
    "    else:\n",
    "        F = np.zeros((3, 3))\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e751400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:40:58.390408Z",
     "iopub.status.busy": "2022-05-17T15:40:58.389375Z",
     "iopub.status.idle": "2022-05-17T15:41:06.940297Z",
     "shell.execute_reply": "2022-05-17T15:41:06.941558Z",
     "shell.execute_reply.started": "2022-05-17T15:37:32.312850Z"
    },
    "papermill": {
     "duration": 8.588906,
     "end_time": "2022-05-17T15:41:06.941817",
     "exception": false,
     "start_time": "2022-05-17T15:40:58.352911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n",
      "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    }
   ],
   "source": [
    "model_type = \"DPT_Large\" \n",
    "depth_estimator = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "depth_estimator.to(device)\n",
    "depth_estimator.eval()\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "transform = midas_transforms.dpt_transform\n",
    "\n",
    "def estimate_depth(filepath, depth_estimator, transform):\n",
    "#     filepath = f'{src}/test_images/{batch_id}/{image_1_id}.png'\n",
    "    img = cv2.imread(filepath)\n",
    "    scale = 640 / max(img.shape[0], img.shape[1]) \n",
    "    w = int(img.shape[1] * scale)\n",
    "    h = int(img.shape[0] * scale)\n",
    "    resized_img = cv2.resize(img, (w, h))\n",
    "    input_batch = transform(resized_img).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mask = depth_estimator(input_batch)\n",
    "\n",
    "        mask = torch.nn.functional.interpolate(\n",
    "            mask.unsqueeze(1),\n",
    "            size=img.shape[:2],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        ).squeeze()\n",
    "\n",
    "    return mask.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feecf7bc",
   "metadata": {
    "papermill": {
     "duration": 0.044081,
     "end_time": "2022-05-17T15:41:07.030931",
     "exception": false,
     "start_time": "2022-05-17T15:41:06.986850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ***Utils***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0985bd63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:41:07.130937Z",
     "iopub.status.busy": "2022-05-17T15:41:07.129873Z",
     "iopub.status.idle": "2022-05-17T15:41:07.143838Z",
     "shell.execute_reply": "2022-05-17T15:41:07.145248Z",
     "shell.execute_reply.started": "2022-05-17T15:37:39.283001Z"
    },
    "papermill": {
     "duration": 0.071046,
     "end_time": "2022-05-17T15:41:07.145575",
     "exception": false,
     "start_time": "2022-05-17T15:41:07.074529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "src = '/kaggle/input/image-matching-challenge-2022/'\n",
    "\n",
    "test_samples = []\n",
    "with open(f'{src}/test.csv') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for i, row in enumerate(reader):\n",
    "        # Skip header.\n",
    "        if i == 0:\n",
    "            continue\n",
    "        test_samples += [row]\n",
    "\n",
    "\n",
    "def FlattenMatrix(M, num_digits=8):\n",
    "    '''Convenience function to write CSV files.'''\n",
    "    \n",
    "    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n",
    "\n",
    "\n",
    "def load_torch_image(fname):\n",
    "    img = cv2.imread(fname)\n",
    "    scale = 840 / max(img.shape[0], img.shape[1]) \n",
    "    w = int(img.shape[1] * scale)\n",
    "    h = int(img.shape[0] * scale)\n",
    "    img = cv2.resize(img, (w, h))\n",
    "    img = K.image_to_tensor(img, False).float() /255.\n",
    "    img = K.color.bgr_to_rgb(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ac52626",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:41:07.249598Z",
     "iopub.status.busy": "2022-05-17T15:41:07.248510Z",
     "iopub.status.idle": "2022-05-17T15:41:07.263362Z",
     "shell.execute_reply": "2022-05-17T15:41:07.264535Z",
     "shell.execute_reply.started": "2022-05-17T15:37:39.295602Z"
    },
    "papermill": {
     "duration": 0.071976,
     "end_time": "2022-05-17T15:41:07.264762",
     "exception": false,
     "start_time": "2022-05-17T15:41:07.192786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_intrinsics(k):\n",
    "    return np.array(k).reshape(3, 3)\n",
    "\n",
    "def get_extrinsics(r, t):\n",
    "    R = np.array(r).reshape(3, 3)\n",
    "    T = np.array(t)\n",
    "    extrinsic = np.eye(4)\n",
    "    extrinsic[:3, :3] = R\n",
    "    extrinsic[:3, 3] = T\n",
    "    return extrinsic\n",
    "\n",
    "def skew(x):\n",
    "    return np.array([[0, -x[2], x[1]],\n",
    "                     [x[2], 0, -x[0]],\n",
    "                     [-x[1], x[0], 0]])\n",
    "\n",
    "def make_K(cam):\n",
    "    return np.array([\n",
    "        [cam.focal_length.detach().numpy()[0, 0], 0, cam.principal_point.detach().numpy()[0, 0]],\n",
    "        [0, cam.focal_length.detach().numpy()[0, 1], cam.principal_point.detach().numpy()[0, 1]],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "def calc_F(cam1, cam2):\n",
    "    \n",
    "    intrinsic1 = get_intrinsics(make_K(cam1))\n",
    "    intrinsic2 = get_intrinsics(make_K(cam2))\n",
    "    extrinsic1 = get_extrinsics(cam1.R.detach().numpy(), cam1.T.detach().numpy())\n",
    "    extrinsic2 = get_extrinsics(cam2.R.detach().numpy(), cam2.T.detach().numpy())\n",
    "    relative = extrinsic2.dot(np.linalg.inv(extrinsic1))\n",
    "\n",
    "    R = relative[:3, :3]\n",
    "    T = relative[:3, 3]\n",
    "    tx = skew(T)\n",
    "    E = np.dot(tx, R)\n",
    "    F = np.linalg.inv(intrinsic2).T.dot(E).dot(np.linalg.inv(intrinsic1))\n",
    "    return F\n",
    "    \n",
    "# intrinsic1 = get_intrinsics(k1)\n",
    "# intrinsic2 = get_intrinsics(k2)\n",
    "\n",
    "# extrinsic1 = get_extrinsics(r1,t1)\n",
    "# extrinsic2 = get_extrinsics(r2,t2)\n",
    "\n",
    "# relative = extrinsic2.dot(np.linalg.inv(extrinsic1))\n",
    "\n",
    "# R = relative[:3, :3]\n",
    "# T = relative[:3, 3]\n",
    "# tx = skew(T)\n",
    "# E = np.dot(tx, R)\n",
    "# F = np.linalg.inv(intrinsic2).T.dot(E).dot(np.linalg.inv(intrinsic1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "970f223f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:41:07.362867Z",
     "iopub.status.busy": "2022-05-17T15:41:07.361853Z",
     "iopub.status.idle": "2022-05-17T15:41:07.374760Z",
     "shell.execute_reply": "2022-05-17T15:41:07.375988Z",
     "shell.execute_reply.started": "2022-05-17T15:37:39.311036Z"
    },
    "papermill": {
     "duration": 0.068127,
     "end_time": "2022-05-17T15:41:07.376204",
     "exception": false,
     "start_time": "2022-05-17T15:41:07.308077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_values():\n",
    "    N=2\n",
    "#     log_R_absolute_init = torch.randn(N, 3, dtype=torch.float32, device=device)\n",
    "    R_I = torch.eye(3,3).unsqueeze(0)\n",
    "    R_absolute_init = torch.cat((R_I, R_I), 0)\n",
    "    T_absolute_init = torch.randn(N, 3, dtype=torch.float32, device=device)\n",
    "    T_absolute_init[0, :] = 0.\n",
    "    focal_length_init = torch.ones((N, 2), dtype=torch.float32, device=device)\n",
    "\n",
    "    R_absolute = R_absolute_init.clone().detach()\n",
    "    R_absolute.requires_grad = True\n",
    "    T_absolute = T_absolute_init.clone().detach()\n",
    "    T_absolute.requires_grad = True\n",
    "    focal_length = focal_length_init.clone().detach()\n",
    "    focal_length.requires_grad = True\n",
    "    \n",
    "    return R_absolute, T_absolute, focal_length\n",
    "\n",
    "def loss_function(xyz_unproj_world, loss_fn=torch.nn.L1Loss(reduction='mean')):\n",
    "    loss = loss_fn(xyz_unproj_world[0], xyz_unproj_world[1])\n",
    "    return loss\n",
    "    \n",
    "def optimization(R_absolute, T_absolute, focal_length, principal_points, image_sizes, xy_depth, n_iter = 2000):\n",
    "    \n",
    "    optimizer = torch.optim.SGD([R_absolute, T_absolute, focal_length], lr=.001, momentum=0.9)\n",
    "    \n",
    "    camera_mask = torch.ones(2, 1, dtype=torch.float32, device=device)\n",
    "    \n",
    "    for it in range(n_iter):\n",
    "        cameras_absolute = PerspectiveCameras(\n",
    "            R = R_absolute,\n",
    "            T = T_absolute,\n",
    "            focal_length = focal_length,\n",
    "            principal_point = principal_points,\n",
    "            image_size = image_sizes,\n",
    "            in_ndc = False,\n",
    "            device = device,\n",
    "        )\n",
    "        optim_one_iter(cameras_absolute, xy_depth, optimizer)\n",
    "        \n",
    "    return cameras_absolute\n",
    "    \n",
    "    \n",
    "def optim_one_iter(cameras, xy_depth, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    xyz_unproj_world = cameras.unproject_points(xy_depth, world_coordinates=True)\n",
    "    loss = loss_function(xyz_unproj_world)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80b60899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:41:07.480981Z",
     "iopub.status.busy": "2022-05-17T15:41:07.479846Z",
     "iopub.status.idle": "2022-05-17T15:41:07.491165Z",
     "shell.execute_reply": "2022-05-17T15:41:07.492282Z",
     "shell.execute_reply.started": "2022-05-17T15:37:39.328406Z"
    },
    "papermill": {
     "duration": 0.071983,
     "end_time": "2022-05-17T15:41:07.492515",
     "exception": false,
     "start_time": "2022-05-17T15:41:07.420532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_xy_depth(dic, device=device):\n",
    "    depth1 = dic[\"depth1\"]\n",
    "    pts1 = dic[\"points1\"].astype(int)\n",
    "    pts1_depth = torch.Tensor(depth1[pts1[:,1], pts1[:,0]])\n",
    "    pts1 = torch.Tensor(pts1)\n",
    "    \n",
    "    depth2 = dic[\"depth2\"]\n",
    "    pts2 = dic[\"points2\"].astype(int)\n",
    "    pts2_depth = torch.Tensor(depth2[pts2[:,1], pts2[:,0]])\n",
    "    pts2 = torch.Tensor(pts2)\n",
    "    \n",
    "    # discard points where depth == 0\n",
    "    valid_idxs = (pts1_depth > 0) & (pts2_depth > 0)\n",
    "    pts1_depth = pts1_depth[valid_idxs]\n",
    "    pts2_depth = pts2_depth[valid_idxs]\n",
    "    pts1_x = pts1[:, 0][valid_idxs]\n",
    "    pts1_y = pts1[:, 1][valid_idxs]\n",
    "    pts1 = torch.cat((pts1_x.unsqueeze(1), pts1_y.unsqueeze(1)), dim=1)\n",
    "    pts2_x = pts2[:, 0][valid_idxs]\n",
    "    pts2_y = pts2[:, 1][valid_idxs]\n",
    "    pts2 = torch.cat((pts2_x.unsqueeze(1), pts2_y.unsqueeze(1)), dim=1)\n",
    "    \n",
    "    xy_depth1 = torch.cat((pts1, pts1_depth.unsqueeze(1)), dim=1).unsqueeze(0)\n",
    "    xy_depth2 = torch.cat((pts2, pts2_depth.unsqueeze(1)), dim=1).unsqueeze(0)\n",
    "    xy_depth = torch.cat((xy_depth1, xy_depth2), dim=0).to(device)\n",
    "    xy_depth.requires_grad = False\n",
    "    return xy_depth\n",
    "\n",
    "def compute_F_from_depth_points_dic(depth_points_dic):\n",
    "    xy_depth = make_xy_depth(depth_points_dic)\n",
    "    \n",
    "    principal_point1 = depth_points_dic[\"principal_point1\"].unsqueeze(0)\n",
    "    principal_point2 = depth_points_dic[\"principal_point2\"].unsqueeze(0)\n",
    "    principal_points = torch.cat((principal_point1, principal_point2), dim=0).to(device)\n",
    "    principal_points.requires_grad = False\n",
    "    \n",
    "    image_size1 = depth_points_dic[\"image_size1\"].unsqueeze(0)\n",
    "    image_size2 = depth_points_dic[\"image_size2\"].unsqueeze(0)\n",
    "    image_sizes = torch.cat((image_size1, image_size2), dim=0).to(device)\n",
    "    image_sizes.requires_grad = False\n",
    "    \n",
    "    log_R_absolute, T_absolute, focal_length = init_values()\n",
    "    \n",
    "    optimized_cameras = optimization(log_R_absolute, T_absolute, focal_length, principal_points, image_sizes, xy_depth, n_iter=2000)\n",
    "    print()\n",
    "    return calc_F(optimized_cameras[0].cpu(), optimized_cameras[1].cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63105b0",
   "metadata": {
    "papermill": {
     "duration": 0.042659,
     "end_time": "2022-05-17T15:41:07.579326",
     "exception": false,
     "start_time": "2022-05-17T15:41:07.536667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ***Inference***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a4f33b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:41:07.673688Z",
     "iopub.status.busy": "2022-05-17T15:41:07.672727Z",
     "iopub.status.idle": "2022-05-17T15:41:51.179981Z",
     "shell.execute_reply": "2022-05-17T15:41:51.180826Z",
     "shell.execute_reply.started": "2022-05-17T15:37:39.345693Z"
    },
    "papermill": {
     "duration": 43.559023,
     "end_time": "2022-05-17T15:41:51.181132",
     "exception": false,
     "start_time": "2022-05-17T15:41:07.622109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\n",
      "  return torch.floor_divide(self, other)\n",
      "../input/pytorch3ddependencies/pytorch3d_dependencies/pytorch3d/transforms/transform3d.py:800: UserWarning: R is not a valid rotation matrix\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "F_dict = {}\n",
    "\n",
    "depth_points_dict = {}\n",
    "\n",
    "import time\n",
    "for i, row in enumerate(test_samples):\n",
    "    sample_id, batch_id, image_1_id, image_2_id = row\n",
    "    # Load the images.\n",
    "    st = time.time()\n",
    "    image1_filepath = f'{src}/test_images/{batch_id}/{image_1_id}.png'\n",
    "    image2_filepath = f'{src}/test_images/{batch_id}/{image_2_id}.png'\n",
    "    image_1 = load_torch_image(image1_filepath).to(device)\n",
    "    image_2 = load_torch_image(image2_filepath).to(device)\n",
    "    \n",
    "    depth1 = estimate_depth(image1_filepath, depth_estimator, transform)\n",
    "    depth2 = estimate_depth(image2_filepath, depth_estimator, transform)\n",
    "    \n",
    "    #LoFTR\n",
    "    mkpts0, mkpts1 = match(image1_filepath, image2_filepath, matcher, device)\n",
    "    \n",
    "#     #SuperGlue\n",
    "#     sg_mkpts0, sg_mkpts1 = superglue_match(f'{src}/test_images/{batch_id}/{image_1_id}.png', f'{src}/test_images/{batch_id}/{image_2_id}.png', superglue, device)\n",
    "    \n",
    "#     mkpts0 = np.vstack((loftr_mkpts0, sg_mkpts0))\n",
    "#     mkpts1 = np.vstack((loftr_mkpts1, sg_mkpts1))\n",
    "\n",
    "    img1_h, img1_w = image_1.shape[2], image_1.shape[3]\n",
    "    img2_h, img2_w = image_2.shape[2], image_2.shape[3]\n",
    "\n",
    "    depth_points_dic = {\n",
    "        \"depth1\": depth1,\n",
    "        \"depth2\": depth2,\n",
    "        \"image1_filepath\": image1_filepath,\n",
    "        \"image2_filepath\": image2_filepath,\n",
    "        \"principal_point1\": torch.Tensor((img1_w/2, img1_h/2)),\n",
    "        \"principal_point2\": torch.Tensor((img2_w/2, img2_h/2)),\n",
    "        \"image_size1\": torch.Tensor((img1_h, img1_w)),\n",
    "        \"image_size2\": torch.Tensor((img2_h, img2_w)),\n",
    "        \"points1\": mkpts0,\n",
    "        \"points2\": mkpts1,\n",
    "    }\n",
    "    \n",
    "    F_dict[sample_id] = compute_F_from_depth_points_dic(depth_points_dic)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('sample_id,fundamental_matrix\\n')\n",
    "    for sample_id, F in F_dict.items():\n",
    "        f.write(f'{sample_id},{FlattenMatrix(F)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f517f66d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:41:51.246707Z",
     "iopub.status.busy": "2022-05-17T15:41:51.245618Z",
     "iopub.status.idle": "2022-05-17T15:41:51.248319Z",
     "shell.execute_reply": "2022-05-17T15:41:51.248843Z",
     "shell.execute_reply.started": "2022-05-17T15:38:04.706930Z"
    },
    "papermill": {
     "duration": 0.036469,
     "end_time": "2022-05-17T15:41:51.249004",
     "exception": false,
     "start_time": "2022-05-17T15:41:51.212535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(optimized_cameras[0].R)\n",
    "# print(optimized_cameras[0].T)\n",
    "# print(optimized_cameras[0].K)\n",
    "# print(optimized_cameras[0].focal_length)\n",
    "# print(optimized_cameras[0].principal_point)\n",
    "# print(optimized_cameras[0].image_size)\n",
    "# print()\n",
    "# print(optimized_cameras[1].R)\n",
    "# print(optimized_cameras[1].T)\n",
    "# print(optimized_cameras[1].K)\n",
    "# print(optimized_cameras[1].focal_length)\n",
    "# print(optimized_cameras[1].principal_point)\n",
    "# print(optimized_cameras[1].image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a7d4987",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:41:51.311069Z",
     "iopub.status.busy": "2022-05-17T15:41:51.310059Z",
     "iopub.status.idle": "2022-05-17T15:41:51.313114Z",
     "shell.execute_reply": "2022-05-17T15:41:51.312576Z",
     "shell.execute_reply.started": "2022-05-17T15:38:04.712682Z"
    },
    "papermill": {
     "duration": 0.035641,
     "end_time": "2022-05-17T15:41:51.313262",
     "exception": false,
     "start_time": "2022-05-17T15:41:51.277621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dir(optimized_cameras[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209b5e00",
   "metadata": {
    "papermill": {
     "duration": 0.02811,
     "end_time": "2022-05-17T15:41:51.370517",
     "exception": false,
     "start_time": "2022-05-17T15:41:51.342407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facce2d8",
   "metadata": {
    "papermill": {
     "duration": 0.028829,
     "end_time": "2022-05-17T15:41:51.427811",
     "exception": false,
     "start_time": "2022-05-17T15:41:51.398982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da89a0a",
   "metadata": {
    "papermill": {
     "duration": 0.028605,
     "end_time": "2022-05-17T15:41:51.486351",
     "exception": false,
     "start_time": "2022-05-17T15:41:51.457746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 130.383604,
   "end_time": "2022-05-17T15:41:53.232736",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-17T15:39:42.849132",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
