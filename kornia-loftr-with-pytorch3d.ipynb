{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ***Install Libs***","metadata":{}},{"cell_type":"code","source":"%%capture\n#dry_run = False\n!pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n!pip install ../input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl\n\n# for depth estimation module\n!mkdir -p /root/.cache/torch/hub/checkpoints\n!cp -r ../input/midasdepthestimation/MiDaS-master  /root/.cache/torch/hub/intel-isl_MiDaS_master\n!cp ../input/midasdepthestimation/dpt_large-midas-2f21e586.pt  /root/.cache/torch/hub/checkpoints/\n!pip install ../input/midasdepthestimation/timm-0.5.4-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:08:39.710924Z","iopub.execute_input":"2022-05-14T11:08:39.71123Z","iopub.status.idle":"2022-05-14T11:09:12.944749Z","shell.execute_reply.started":"2022-05-14T11:08:39.711181Z","shell.execute_reply":"2022-05-14T11:09:12.943771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Import dependencies***","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport csv\nfrom glob import glob\nimport gc\nimport random\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport cv2\nimport torch\n\n# for LoFTR\nimport kornia\nfrom kornia_moons.feature import *\nimport kornia as K\nimport kornia.feature as KF\n\n# for depth-estimation\nimport timm","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:09:12.950953Z","iopub.execute_input":"2022-05-14T11:09:12.951455Z","iopub.status.idle":"2022-05-14T11:09:14.379459Z","shell.execute_reply.started":"2022-05-14T11:09:12.951416Z","shell.execute_reply":"2022-05-14T11:09:14.378532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys.path.append(\"../input/imcutils\")\nfrom imc_metric import EvaluateSubmission, ReadCovisibilityData, FlattenMatrix, LoadCalibration\n\nsys.path.append(\"../input/super-glue-pretrained-network\")\nfrom models.matching import Matching as SuperGlue\nfrom models.utils import (compute_pose_error, compute_epipolar_error,\n                          estimate_pose, make_matching_plot,\n                          error_colormap, AverageTimer, pose_auc, read_image,\n                          rotate_intrinsics, rotate_pose_inplane,\n                          scale_intrinsics)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:09:14.381003Z","iopub.execute_input":"2022-05-14T11:09:14.381573Z","iopub.status.idle":"2022-05-14T11:09:14.412645Z","shell.execute_reply.started":"2022-05-14T11:09:14.381543Z","shell.execute_reply":"2022-05-14T11:09:14.411905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for pytorch3d\nsys.path.append(\"../input/pytorch3ddependencies/pytorch3d_dependencies\")\nos.environ[\"CUB_HOME\"] = \"../input/pytorch3ddependencies/pytorch3d_dependencies/cub-1.10.0\"\nimport pytorch3d","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:09:14.415203Z","iopub.execute_input":"2022-05-14T11:09:14.415736Z","iopub.status.idle":"2022-05-14T11:09:14.437928Z","shell.execute_reply.started":"2022-05-14T11:09:14.415698Z","shell.execute_reply":"2022-05-14T11:09:14.437302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch3d.renderer.cameras import (\n    PerspectiveCameras,\n)\nfrom pytorch3d.transforms.so3 import (\n    so3_exp_map\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:09:14.439213Z","iopub.execute_input":"2022-05-14T11:09:14.439623Z","iopub.status.idle":"2022-05-14T11:09:14.650444Z","shell.execute_reply.started":"2022-05-14T11:09:14.439587Z","shell.execute_reply":"2022-05-14T11:09:14.649636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Model***","metadata":{}},{"cell_type":"code","source":"# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n# LoFTR\nmatcher = KF.LoFTR(pretrained=None)\nmatcher.load_state_dict(torch.load(\"../input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict'])\nmatcher = matcher.to(device)\nmatcher.eval()\nprint()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:09:14.65198Z","iopub.execute_input":"2022-05-14T11:09:14.652399Z","iopub.status.idle":"2022-05-14T11:09:17.201571Z","shell.execute_reply.started":"2022-05-14T11:09:14.652334Z","shell.execute_reply":"2022-05-14T11:09:17.200729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Super Glue\nconfig = {\n    \"superpoint\": {\n        \"nms_radius\": 4,\n        \"keypoint_threshold\": 0.005,\n        \"max_keypoints\": 1024\n    },\n    \"superglue\": {\n        \"weights\": \"outdoor\",\n        \"sinkhorn_iterations\": 20,\n        \"match_threshold\": 0.2,\n    }\n}\nsuperglue = SuperGlue(config).eval().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:09:17.202931Z","iopub.execute_input":"2022-05-14T11:09:17.203387Z","iopub.status.idle":"2022-05-14T11:09:17.424639Z","shell.execute_reply.started":"2022-05-14T11:09:17.203333Z","shell.execute_reply":"2022-05-14T11:09:17.42392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def match(img_path0, img_path1, matcher, device=device):\n    img0 = load_torch_image(img_path0)\n    img1 = load_torch_image(img_path1)\n        \n    input_dict = {\"image0\": K.color.rgb_to_grayscale(img0).to(device), \n                  \"image1\": K.color.rgb_to_grayscale(img1).to(device)}\n    \n    with torch.no_grad():\n        correspondences = matcher(input_dict)\n        \n    mkpts0 = correspondences['keypoints0'].cpu().numpy()\n    mkpts1 = correspondences['keypoints1'].cpu().numpy()\n        \n    return mkpts0, mkpts1\n\ndef superglue_match(img_path0, img_path1, matcher, device=device):\n    resize = [-1, ]\n    resize_float = True\n    image_1, inp_1, scales_1 = read_image(img_path0, device, resize, 0, resize_float)\n    image_2, inp_2, scales_2 = read_image(img_path1, device, resize, 0, resize_float)\n    \n    pred = matcher({\"image0\": inp_1, \"image1\": inp_2})\n    pred = {k: v[0].detach().cpu().numpy() for k, v in pred.items()}\n    kpts1, kpts2 = pred[\"keypoints0\"], pred[\"keypoints1\"]\n    matches, conf = pred[\"matches0\"], pred[\"matching_scores0\"]\n    \n    valid = matches > -1\n    mkpts1 = kpts1[valid]\n    mkpts2 = kpts2[matches[valid]]\n    \n    return mkpts1, mkpts2\n\ndef get_F_matrix(mkpts0, mkpts1):\n    # Make sure we do not trigger an exception here.\n    if len(mkpts0) > 8:\n        F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.200, 0.9999, 250000)\n        assert F.shape == (3, 3), 'Malformed F?'\n    else:\n        F = np.zeros((3, 3))\n    return F","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:09:17.425975Z","iopub.execute_input":"2022-05-14T11:09:17.42629Z","iopub.status.idle":"2022-05-14T11:09:17.438538Z","shell.execute_reply.started":"2022-05-14T11:09:17.426245Z","shell.execute_reply":"2022-05-14T11:09:17.437649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_type = \"DPT_Large\" \ndepth_estimator = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\ndepth_estimator.to(device)\ndepth_estimator.eval()\nmidas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\ntransform = midas_transforms.dpt_transform\n\ndef estimate_depth(filepath, depth_estimator, transform):\n#     filepath = f'{src}/test_images/{batch_id}/{image_1_id}.png'\n    img = cv2.imread(filepath)\n    scale = 640 / max(img.shape[0], img.shape[1]) \n    w = int(img.shape[1] * scale)\n    h = int(img.shape[0] * scale)\n    resized_img = cv2.resize(img, (w, h))\n    input_batch = transform(resized_img).to(device)\n    \n    with torch.no_grad():\n        mask = depth_estimator(input_batch)\n\n        mask = torch.nn.functional.interpolate(\n            mask.unsqueeze(1),\n            size=img.shape[:2],\n            mode=\"bicubic\",\n            align_corners=False,\n        ).squeeze()\n\n    return mask.cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:09:17.439975Z","iopub.execute_input":"2022-05-14T11:09:17.440444Z","iopub.status.idle":"2022-05-14T11:09:24.763025Z","shell.execute_reply.started":"2022-05-14T11:09:17.440407Z","shell.execute_reply":"2022-05-14T11:09:24.762136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ***Utils***","metadata":{}},{"cell_type":"code","source":"src = '/kaggle/input/image-matching-challenge-2022/'\n\ntest_samples = []\nwith open(f'{src}/test.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        test_samples += [row]\n\n\ndef FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    \n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n\n\ndef load_torch_image(fname):\n    img = cv2.imread(fname)\n    scale = 840 / max(img.shape[0], img.shape[1]) \n    w = int(img.shape[1] * scale)\n    h = int(img.shape[0] * scale)\n    img = cv2.resize(img, (w, h))\n    img = K.image_to_tensor(img, False).float() /255.\n    img = K.color.bgr_to_rgb(img)\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:09:24.764325Z","iopub.execute_input":"2022-05-14T11:09:24.765198Z","iopub.status.idle":"2022-05-14T11:09:24.776889Z","shell.execute_reply.started":"2022-05-14T11:09:24.765152Z","shell.execute_reply":"2022-05-14T11:09:24.775946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Inference***","metadata":{}},{"cell_type":"code","source":"F_dict = {}\n\ndepth_points_dict = {}\n\nimport time\nfor i, row in enumerate(test_samples):\n    sample_id, batch_id, image_1_id, image_2_id = row\n    # Load the images.\n    st = time.time()\n    image1_filepath = f'{src}/test_images/{batch_id}/{image_1_id}.png'\n    image2_filepath = f'{src}/test_images/{batch_id}/{image_2_id}.png'\n    image_1 = load_torch_image(image1_filepath).to(device)\n    image_2 = load_torch_image(image2_filepath).to(device)\n    \n    depth1 = estimate_depth(image1_filepath, depth_estimator, transform)\n    depth2 = estimate_depth(image2_filepath, depth_estimator, transform)\n    \n    #LoFTR\n    mkpts0, mkpts1 = match(image1_filepath, image2_filepath, matcher, device)\n    \n#     #SuperGlue\n#     sg_mkpts0, sg_mkpts1 = superglue_match(f'{src}/test_images/{batch_id}/{image_1_id}.png', f'{src}/test_images/{batch_id}/{image_2_id}.png', superglue, device)\n    \n#     mkpts0 = np.vstack((loftr_mkpts0, sg_mkpts0))\n#     mkpts1 = np.vstack((loftr_mkpts1, sg_mkpts1))\n\n    img1_w, img1_h = image_1.shape[2], image_1.shape[3]\n    img2_w, img2_h = image_2.shape[2], image_2.shape[3]\n\n    depth_points_dict[sample_id] = {\n        \"depth1\": depth1,\n        \"depth2\": depth2,\n        \"image1_filepath\": image1_filepath,\n        \"image2_filepath\": image2_filepath,\n        \"principal_point1\": torch.Tensor((img1_w, img1_h)),\n        \"principal_point2\": torch.Tensor((img2_w, img2_h)),\n        \"points1\": mkpts0,\n        \"points2\": mkpts1,\n    }\n    \n    if len(mkpts0) > 7:\n        F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.200, 0.9999, 250000)\n        inliers = inliers > 0\n        assert F.shape == (3, 3), 'Malformed F?'\n        F_dict[sample_id] = F\n    else:\n        F_dict[sample_id] = np.zeros((3, 3))\n        continue\n    gc.collect()\n    nd = time.time()    \n    if (i < 3):\n        print(\"Running time: \", nd - st, \" s\")\n        draw_LAF_matches(\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0).view(1,-1, 2),\n                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1)),\n\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1).view(1,-1, 2),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1)),\n        torch.arange(mkpts0.shape[0]).view(-1,1).repeat(1,2),\n        K.tensor_to_image(image_1),\n        K.tensor_to_image(image_2),\n        inliers,\n        draw_dict={'inlier_color': (0.2, 1, 0.2),\n                   'tentative_color': None, \n                   'feature_color': (0.2, 0.5, 1), 'vertical': False})\n    \nwith open('submission.csv', 'w') as f:\n    f.write('sample_id,fundamental_matrix\\n')\n    for sample_id, F in F_dict.items():\n        f.write(f'{sample_id},{FlattenMatrix(F)}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:09:24.780148Z","iopub.execute_input":"2022-05-14T11:09:24.780965Z","iopub.status.idle":"2022-05-14T11:09:33.602866Z","shell.execute_reply.started":"2022-05-14T11:09:24.780923Z","shell.execute_reply":"2022-05-14T11:09:33.601908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_values():\n    N=2\n    log_R_absolute_init = torch.randn(N, 3, dtype=torch.float32, device=device)\n    T_absolute_init = torch.randn(N, 3, dtype=torch.float32, device=device)\n    focal_length_init = torch.ones((N, 2), dtype=torch.float32, device=device)\n\n    log_R_absolute_init[0, :] = 0.\n    T_absolute_init[0, :] = 0.\n\n    log_R_absolute = log_R_absolute_init.clone().detach()\n    log_R_absolute.requires_grad = True\n    T_absolute = T_absolute_init.clone().detach()\n    T_absolute.requires_grad = True\n    focal_length = focal_length_init.clone().detach()\n    focal_length.requires_grad = True\n    \n    return log_R_absolute, T_absolute, focal_length\n\ndef loss_function(xyz_unproj_world, loss_fn=torch.nn.L1Loss()):\n    return loss_fn(xyz_unproj_world[0], xyz_unproj_world[1])\n    \ndef optimization(log_R_absolute, T_absolute, focal_length, principal_points, xy_depth, n_iter = 2000):\n    \n    optimizer = torch.optim.SGD([log_R_absolute, T_absolute, focal_length], lr=.1, momentum=0.9)\n    \n    camera_mask = torch.ones(2, 1, dtype=torch.float32, device=device)\n    camera_mask[0] = 0.\n    \n#     init_R = torch.eye(3,3).unsqueeze(0)\n#     init_Rs = torch.cat((init_R, init_R), 0)\n#     init_T = torch.Tensor((0,0,0)).unsqueeze(0)\n#     init_Ts = torch.cat((init_T, init_T), 0)\n#     cameras = PerspectiveCameras(R=init_Rs, T=init_Ts)\n    \n    for it in range(n_iter):\n        R_absolute = so3_exp_map(log_R_absolute * camera_mask)\n        print(R_absolute)\n        cameras_absolute = PerspectiveCameras(\n            R = R_absolute,\n            T = T_absolute * camera_mask,\n            focal_length = focal_length,\n            principal_point = principal_points,\n            device = device,\n        )\n        optim_one_iter(cameras_absolute, xy_depth, optimizer)\n        \n    return cameras_absolute\n    \n    \ndef optim_one_iter(cameras, xy_depth, optimizer):\n    optimizer.zero_grad()\n    \n    xyz_unproj_world = cameras.unproject_points(xy_depth, world_coordinates=True)\n    loss = loss_function(xyz_unproj_world)\n    loss.backward()\n    optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:15:55.051222Z","iopub.execute_input":"2022-05-14T11:15:55.051518Z","iopub.status.idle":"2022-05-14T11:15:55.063481Z","shell.execute_reply.started":"2022-05-14T11:15:55.051482Z","shell.execute_reply":"2022-05-14T11:15:55.062753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_xy_depth(dic, device=device):\n    depth1 = dic[\"depth1\"]\n    pts1 = dic[\"points1\"].astype(int)\n    pts1_depth = torch.Tensor(depth1[pts1[:,1], pts1[:,0]])\n    pts1 = torch.Tensor(pts1)\n    \n    depth2 = dic[\"depth2\"]\n    pts2 = dic[\"points2\"].astype(int)\n    pts2_depth = torch.Tensor(depth2[pts2[:,1], pts2[:,0]])\n    pts2 = torch.Tensor(pts2)\n    \n    xy_depth1 = torch.cat((pts1, pts1_depth.unsqueeze(1)), dim=1).unsqueeze(0)\n    xy_depth2 = torch.cat((pts2, pts2_depth.unsqueeze(1)), dim=1).unsqueeze(0)\n    xy_depth = torch.cat((xy_depth1, xy_depth2), dim=0).to(device)\n    xy_depth.requires_grad = False\n    return xy_depth","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:15:55.488999Z","iopub.execute_input":"2022-05-14T11:15:55.489249Z","iopub.status.idle":"2022-05-14T11:15:55.49724Z","shell.execute_reply.started":"2022-05-14T11:15:55.489222Z","shell.execute_reply":"2022-05-14T11:15:55.496136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for pair_id, depth_points_dic in depth_points_dict.items():\n    \n    xy_depth = make_xy_depth(depth_points_dic)\n    \n    principal_point1 = depth_points_dic[\"principal_point1\"].unsqueeze(0)\n    principal_point2 = depth_points_dic[\"principal_point2\"].unsqueeze(0)\n    principal_points = torch.cat((principal_point1, principal_point2), dim=0).to(device)\n    principal_points.requires_grad = False\n    \n    log_R_absolute, T_absolute, focal_length = init_values()\n    \n    optimized_cameras = optimization(log_R_absolute, T_absolute, focal_length, principal_points, xy_depth)\n    \n    break","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:15:57.209159Z","iopub.execute_input":"2022-05-14T11:15:57.209899Z","iopub.status.idle":"2022-05-14T11:15:57.223058Z","shell.execute_reply.started":"2022-05-14T11:15:57.209859Z","shell.execute_reply":"2022-05-14T11:15:57.222077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimized_cameras[1].R","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:13:30.261828Z","iopub.execute_input":"2022-05-14T11:13:30.262467Z","iopub.status.idle":"2022-05-14T11:13:30.270011Z","shell.execute_reply.started":"2022-05-14T11:13:30.262429Z","shell.execute_reply":"2022-05-14T11:13:30.269295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir(optimized_cameras[0])","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:12:48.816343Z","iopub.execute_input":"2022-05-14T11:12:48.816857Z","iopub.status.idle":"2022-05-14T11:12:48.829121Z","shell.execute_reply.started":"2022-05-14T11:12:48.81682Z","shell.execute_reply":"2022-05-14T11:12:48.828232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}