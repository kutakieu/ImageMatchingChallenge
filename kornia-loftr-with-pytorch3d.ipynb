{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c39960",
   "metadata": {
    "papermill": {
     "duration": 0.022343,
     "end_time": "2022-05-18T14:14:27.410703",
     "exception": false,
     "start_time": "2022-05-18T14:14:27.388360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ***Install Libs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57339087",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:14:27.463552Z",
     "iopub.status.busy": "2022-05-18T14:14:27.462955Z",
     "iopub.status.idle": "2022-05-18T14:16:11.558163Z",
     "shell.execute_reply": "2022-05-18T14:16:11.557506Z",
     "shell.execute_reply.started": "2022-05-18T13:15:34.159812Z"
    },
    "papermill": {
     "duration": 104.125332,
     "end_time": "2022-05-18T14:16:11.558315",
     "exception": false,
     "start_time": "2022-05-18T14:14:27.432983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#dry_run = False\n",
    "!pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n",
    "!pip install ../input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl\n",
    "\n",
    "# for depth estimation module\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp -r ../input/midasdepthestimation/MiDaS-master  /root/.cache/torch/hub/intel-isl_MiDaS_master\n",
    "!cp ../input/midasdepthestimation/dpt_large-midas-2f21e586.pt  /root/.cache/torch/hub/checkpoints/\n",
    "!pip install ../input/midasdepthestimation/timm-0.5.4-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d97d02",
   "metadata": {
    "papermill": {
     "duration": 0.020638,
     "end_time": "2022-05-18T14:16:11.600295",
     "exception": false,
     "start_time": "2022-05-18T14:16:11.579657",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ***Import dependencies***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6911fd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:16:11.647908Z",
     "iopub.status.busy": "2022-05-18T14:16:11.647024Z",
     "iopub.status.idle": "2022-05-18T14:16:14.969512Z",
     "shell.execute_reply": "2022-05-18T14:16:14.969010Z",
     "shell.execute_reply.started": "2022-05-18T13:17:26.516871Z"
    },
    "papermill": {
     "duration": 3.348834,
     "end_time": "2022-05-18T14:16:14.969655",
     "exception": false,
     "start_time": "2022-05-18T14:16:11.620821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "from glob import glob\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# for LoFTR\n",
    "import kornia\n",
    "from kornia_moons.feature import *\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "# for depth-estimation\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60aaa201",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:16:15.017036Z",
     "iopub.status.busy": "2022-05-18T14:16:15.013636Z",
     "iopub.status.idle": "2022-05-18T14:16:15.063277Z",
     "shell.execute_reply": "2022-05-18T14:16:15.062816Z",
     "shell.execute_reply.started": "2022-05-18T13:17:30.659914Z"
    },
    "papermill": {
     "duration": 0.073091,
     "end_time": "2022-05-18T14:16:15.063413",
     "exception": false,
     "start_time": "2022-05-18T14:16:14.990322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"../input/imcutils\")\n",
    "from imc_metric import EvaluateSubmission, ReadCovisibilityData, FlattenMatrix, LoadCalibration\n",
    "\n",
    "sys.path.append(\"../input/super-glue-pretrained-network\")\n",
    "from models.matching import Matching as SuperGlue\n",
    "from models.utils import (compute_pose_error, compute_epipolar_error,\n",
    "                          estimate_pose, make_matching_plot,\n",
    "                          error_colormap, AverageTimer, pose_auc, read_image,\n",
    "                          rotate_intrinsics, rotate_pose_inplane,\n",
    "                          scale_intrinsics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ef3cbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:16:15.108821Z",
     "iopub.status.busy": "2022-05-18T14:16:15.108186Z",
     "iopub.status.idle": "2022-05-18T14:16:15.123515Z",
     "shell.execute_reply": "2022-05-18T14:16:15.123063Z",
     "shell.execute_reply.started": "2022-05-18T13:17:30.723072Z"
    },
    "papermill": {
     "duration": 0.03972,
     "end_time": "2022-05-18T14:16:15.123649",
     "exception": false,
     "start_time": "2022-05-18T14:16:15.083929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for pytorch3d\n",
    "sys.path.append(\"../input/pytorch3ddependencies/pytorch3d_dependencies\")\n",
    "os.environ[\"CUB_HOME\"] = \"../input/pytorch3ddependencies/pytorch3d_dependencies/cub-1.10.0\"\n",
    "import pytorch3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e62b256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:16:15.172682Z",
     "iopub.status.busy": "2022-05-18T14:16:15.172093Z",
     "iopub.status.idle": "2022-05-18T14:16:15.704812Z",
     "shell.execute_reply": "2022-05-18T14:16:15.704284Z",
     "shell.execute_reply.started": "2022-05-18T13:17:30.753041Z"
    },
    "papermill": {
     "duration": 0.558458,
     "end_time": "2022-05-18T14:16:15.704966",
     "exception": false,
     "start_time": "2022-05-18T14:16:15.146508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch3d.renderer.cameras import (\n",
    "    PerspectiveCameras,\n",
    ")\n",
    "from pytorch3d.transforms.so3 import (\n",
    "    so3_exp_map\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a64821e",
   "metadata": {
    "papermill": {
     "duration": 0.020295,
     "end_time": "2022-05-18T14:16:15.746557",
     "exception": false,
     "start_time": "2022-05-18T14:16:15.726262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ***Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99ed4aad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:16:15.850608Z",
     "iopub.status.busy": "2022-05-18T14:16:15.849909Z",
     "iopub.status.idle": "2022-05-18T14:16:20.518023Z",
     "shell.execute_reply": "2022-05-18T14:16:20.517584Z",
     "shell.execute_reply.started": "2022-05-18T13:17:31.482652Z"
    },
    "papermill": {
     "duration": 4.751193,
     "end_time": "2022-05-18T14:16:20.518145",
     "exception": false,
     "start_time": "2022-05-18T14:16:15.766952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# LoFTR\n",
    "matcher = KF.LoFTR(pretrained=None)\n",
    "matcher.load_state_dict(torch.load(\"../input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict'])\n",
    "matcher = matcher.to(device)\n",
    "matcher.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db706d4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:16:20.567055Z",
     "iopub.status.busy": "2022-05-18T14:16:20.566263Z",
     "iopub.status.idle": "2022-05-18T14:16:21.287210Z",
     "shell.execute_reply": "2022-05-18T14:16:21.287744Z",
     "shell.execute_reply.started": "2022-05-18T13:17:36.442187Z"
    },
    "papermill": {
     "duration": 0.748589,
     "end_time": "2022-05-18T14:16:21.287913",
     "exception": false,
     "start_time": "2022-05-18T14:16:20.539324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SuperPoint model\n",
      "Loaded SuperGlue model (\"outdoor\" weights)\n"
     ]
    }
   ],
   "source": [
    "# Super Glue\n",
    "config = {\n",
    "    \"superpoint\": {\n",
    "        \"nms_radius\": 4,\n",
    "        \"keypoint_threshold\": 0.005,\n",
    "        \"max_keypoints\": 1024\n",
    "    },\n",
    "    \"superglue\": {\n",
    "        \"weights\": \"outdoor\",\n",
    "        \"sinkhorn_iterations\": 20,\n",
    "        \"match_threshold\": 0.2,\n",
    "    }\n",
    "}\n",
    "superglue = SuperGlue(config).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f08883ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:16:21.349298Z",
     "iopub.status.busy": "2022-05-18T14:16:21.348595Z",
     "iopub.status.idle": "2022-05-18T14:16:21.352911Z",
     "shell.execute_reply": "2022-05-18T14:16:21.352365Z",
     "shell.execute_reply.started": "2022-05-18T13:17:37.301718Z"
    },
    "papermill": {
     "duration": 0.04149,
     "end_time": "2022-05-18T14:16:21.353044",
     "exception": false,
     "start_time": "2022-05-18T14:16:21.311554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def match(img_path0, img_path1, matcher, device=device):\n",
    "    img0 = load_torch_image(img_path0)\n",
    "    img1 = load_torch_image(img_path1)\n",
    "        \n",
    "    input_dict = {\"image0\": K.color.rgb_to_grayscale(img0).to(device), \n",
    "                  \"image1\": K.color.rgb_to_grayscale(img1).to(device)}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correspondences = matcher(input_dict)\n",
    "        \n",
    "    mkpts0 = correspondences['keypoints0'].cpu().numpy()\n",
    "    mkpts1 = correspondences['keypoints1'].cpu().numpy()\n",
    "    confidence = correspondences['confidence'].cpu().numpy()\n",
    "        \n",
    "    return mkpts0, mkpts1, confidence\n",
    "\n",
    "def superglue_match(img_path0, img_path1, matcher, device=device):\n",
    "    resize = [-1, ]\n",
    "    resize_float = True\n",
    "    image_1, inp_1, scales_1 = read_image(img_path0, device, resize, 0, resize_float)\n",
    "    image_2, inp_2, scales_2 = read_image(img_path1, device, resize, 0, resize_float)\n",
    "    \n",
    "    pred = matcher({\"image0\": inp_1, \"image1\": inp_2})\n",
    "    pred = {k: v[0].detach().cpu().numpy() for k, v in pred.items()}\n",
    "    kpts1, kpts2 = pred[\"keypoints0\"], pred[\"keypoints1\"]\n",
    "    matches, conf = pred[\"matches0\"], pred[\"matching_scores0\"]\n",
    "    \n",
    "    valid = matches > -1\n",
    "    mkpts1 = kpts1[valid]\n",
    "    mkpts2 = kpts2[matches[valid]]\n",
    "    \n",
    "    return mkpts1, mkpts2\n",
    "\n",
    "def get_F_matrix(mkpts0, mkpts1):\n",
    "    # Make sure we do not trigger an exception here.\n",
    "    if len(mkpts0) > 8:\n",
    "        F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.200, 0.9999, 250000)\n",
    "        assert F.shape == (3, 3), 'Malformed F?'\n",
    "    else:\n",
    "        F = np.zeros((3, 3))\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af9ccb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:16:21.407939Z",
     "iopub.status.busy": "2022-05-18T14:16:21.407123Z",
     "iopub.status.idle": "2022-05-18T14:16:28.292178Z",
     "shell.execute_reply": "2022-05-18T14:16:28.292608Z",
     "shell.execute_reply.started": "2022-05-18T13:17:37.321254Z"
    },
    "papermill": {
     "duration": 6.916289,
     "end_time": "2022-05-18T14:16:28.292769",
     "exception": false,
     "start_time": "2022-05-18T14:16:21.376480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n",
      "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    }
   ],
   "source": [
    "model_type = \"DPT_Large\" \n",
    "depth_estimator = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "depth_estimator.to(device)\n",
    "depth_estimator.eval()\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "transform = midas_transforms.dpt_transform\n",
    "\n",
    "def estimate_depth(filepath, depth_estimator, transform):\n",
    "#     filepath = f'{src}/test_images/{batch_id}/{image_1_id}.png'\n",
    "    img = cv2.imread(filepath)\n",
    "    scale = 640 / max(img.shape[0], img.shape[1]) \n",
    "    w = int(img.shape[1] * scale)\n",
    "    h = int(img.shape[0] * scale)\n",
    "    resized_img = cv2.resize(img, (w, h))\n",
    "    input_batch = transform(resized_img).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mask = depth_estimator(input_batch)\n",
    "\n",
    "        mask = torch.nn.functional.interpolate(\n",
    "            mask.unsqueeze(1),\n",
    "            size=img.shape[:2],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        ).squeeze()\n",
    "\n",
    "    return mask.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf965a2",
   "metadata": {
    "papermill": {
     "duration": 0.022061,
     "end_time": "2022-05-18T14:16:28.338981",
     "exception": false,
     "start_time": "2022-05-18T14:16:28.316920",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ***Utils***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c013fae3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:16:28.392956Z",
     "iopub.status.busy": "2022-05-18T14:16:28.392249Z",
     "iopub.status.idle": "2022-05-18T14:16:28.399029Z",
     "shell.execute_reply": "2022-05-18T14:16:28.398560Z",
     "shell.execute_reply.started": "2022-05-18T13:17:45.948150Z"
    },
    "papermill": {
     "duration": 0.037573,
     "end_time": "2022-05-18T14:16:28.399146",
     "exception": false,
     "start_time": "2022-05-18T14:16:28.361573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "src = '/kaggle/input/image-matching-challenge-2022/'\n",
    "\n",
    "test_samples = []\n",
    "with open(f'{src}/test.csv') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for i, row in enumerate(reader):\n",
    "        # Skip header.\n",
    "        if i == 0:\n",
    "            continue\n",
    "        test_samples += [row]\n",
    "\n",
    "\n",
    "def FlattenMatrix(M, num_digits=8):\n",
    "    '''Convenience function to write CSV files.'''\n",
    "    \n",
    "    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n",
    "\n",
    "\n",
    "def load_torch_image(fname):\n",
    "    img = cv2.imread(fname)\n",
    "    scale = 840 / max(img.shape[0], img.shape[1]) \n",
    "    w = int(img.shape[1] * scale)\n",
    "    h = int(img.shape[0] * scale)\n",
    "    img = cv2.resize(img, (w, h))\n",
    "    img = K.image_to_tensor(img, False).float() /255.\n",
    "    img = K.color.bgr_to_rgb(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1c09638",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:16:28.457606Z",
     "iopub.status.busy": "2022-05-18T14:16:28.456707Z",
     "iopub.status.idle": "2022-05-18T14:16:28.458488Z",
     "shell.execute_reply": "2022-05-18T14:16:28.458859Z",
     "shell.execute_reply.started": "2022-05-18T13:17:45.965213Z"
    },
    "papermill": {
     "duration": 0.037046,
     "end_time": "2022-05-18T14:16:28.458996",
     "exception": false,
     "start_time": "2022-05-18T14:16:28.421950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_intrinsics(k):\n",
    "    return np.array(k).reshape(3, 3)\n",
    "\n",
    "def get_extrinsics(r, t):\n",
    "    R = np.array(r).reshape(3, 3)\n",
    "    T = np.array(t)\n",
    "    extrinsic = np.eye(4)\n",
    "    extrinsic[:3, :3] = R\n",
    "    extrinsic[:3, 3] = T\n",
    "    return extrinsic\n",
    "\n",
    "def skew(x):\n",
    "    return np.array([[0, -x[2], x[1]],\n",
    "                     [x[2], 0, -x[0]],\n",
    "                     [-x[1], x[0], 0]])\n",
    "\n",
    "def make_K(cam):\n",
    "    return np.array([\n",
    "        [cam.focal_length.detach().numpy()[0, 0], 0, cam.principal_point.detach().numpy()[0, 0]],\n",
    "        [0, cam.focal_length.detach().numpy()[0, 1], cam.principal_point.detach().numpy()[0, 1]],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "def calc_F(cam1, cam2):\n",
    "    \n",
    "    intrinsic1 = get_intrinsics(make_K(cam1))\n",
    "    intrinsic2 = get_intrinsics(make_K(cam2))\n",
    "    extrinsic1 = get_extrinsics(cam1.R.detach().numpy(), cam1.T.detach().numpy())\n",
    "    extrinsic2 = get_extrinsics(cam2.R.detach().numpy(), cam2.T.detach().numpy())\n",
    "    relative = extrinsic2.dot(np.linalg.inv(extrinsic1))\n",
    "\n",
    "    R = relative[:3, :3]\n",
    "    T = relative[:3, 3]\n",
    "    tx = skew(T)\n",
    "    E = np.dot(tx, R)\n",
    "    F = np.linalg.inv(intrinsic2).T.dot(E).dot(np.linalg.inv(intrinsic1))\n",
    "    return F\n",
    "    \n",
    "# intrinsic1 = get_intrinsics(k1)\n",
    "# intrinsic2 = get_intrinsics(k2)\n",
    "\n",
    "# extrinsic1 = get_extrinsics(r1,t1)\n",
    "# extrinsic2 = get_extrinsics(r2,t2)\n",
    "\n",
    "# relative = extrinsic2.dot(np.linalg.inv(extrinsic1))\n",
    "\n",
    "# R = relative[:3, :3]\n",
    "# T = relative[:3, 3]\n",
    "# tx = skew(T)\n",
    "# E = np.dot(tx, R)\n",
    "# F = np.linalg.inv(intrinsic2).T.dot(E).dot(np.linalg.inv(intrinsic1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "100335aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:16:28.506330Z",
     "iopub.status.busy": "2022-05-18T14:16:28.505424Z",
     "iopub.status.idle": "2022-05-18T14:16:28.517387Z",
     "shell.execute_reply": "2022-05-18T14:16:28.516967Z",
     "shell.execute_reply.started": "2022-05-18T13:17:45.985627Z"
    },
    "papermill": {
     "duration": 0.036578,
     "end_time": "2022-05-18T14:16:28.517526",
     "exception": false,
     "start_time": "2022-05-18T14:16:28.480948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_values():\n",
    "    N=2\n",
    "#     log_R_absolute_init = torch.randn(N, 3, dtype=torch.float32, device=device)\n",
    "    R_I = torch.eye(3,3).unsqueeze(0)\n",
    "    R_absolute_init = torch.cat((R_I, R_I), 0)\n",
    "    T_absolute_init = torch.randn(N, 3, dtype=torch.float32, device=device)\n",
    "    T_absolute_init[0, :] = 0.\n",
    "    focal_length_init = torch.ones((N, 2), dtype=torch.float32, device=device)\n",
    "\n",
    "    R_absolute = R_absolute_init.clone().detach()\n",
    "    R_absolute.requires_grad = True\n",
    "    T_absolute = T_absolute_init.clone().detach()\n",
    "    T_absolute.requires_grad = True\n",
    "    focal_length = focal_length_init.clone().detach()\n",
    "    focal_length.requires_grad = True\n",
    "    \n",
    "    return R_absolute, T_absolute, focal_length\n",
    "\n",
    "def loss_function(xyz_unproj_world, loss_fn=torch.nn.L1Loss(reduction='mean')):\n",
    "    loss = loss_fn(xyz_unproj_world[0], xyz_unproj_world[1])\n",
    "    return loss\n",
    "    \n",
    "def optimization(R_absolute, T_absolute, focal_length, principal_points, image_sizes, xy_depth, n_iter = 2000):\n",
    "    \n",
    "    optimizer = torch.optim.SGD([R_absolute, T_absolute, focal_length], lr=1, momentum=0.9)\n",
    "    \n",
    "    camera_mask = torch.ones(2, 1, dtype=torch.float32, device=device)\n",
    "    \n",
    "    for it in range(n_iter):\n",
    "        cameras_absolute = PerspectiveCameras(\n",
    "            R = R_absolute,\n",
    "            T = T_absolute,\n",
    "            focal_length = focal_length,\n",
    "            principal_point = principal_points,\n",
    "            image_size = image_sizes,\n",
    "            in_ndc = False,\n",
    "            device = device,\n",
    "        )\n",
    "        optim_one_iter(cameras_absolute, xy_depth, optimizer)\n",
    "        \n",
    "    return cameras_absolute\n",
    "    \n",
    "    \n",
    "def optim_one_iter(cameras, xy_depth, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    xyz_unproj_world = cameras.unproject_points(xy_depth, world_coordinates=True)\n",
    "    loss = loss_function(xyz_unproj_world, loss_fn=torch.nn.MSELoss(reduction='mean'))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8788df28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:16:28.576609Z",
     "iopub.status.busy": "2022-05-18T14:16:28.575688Z",
     "iopub.status.idle": "2022-05-18T14:16:28.578144Z",
     "shell.execute_reply": "2022-05-18T14:16:28.577700Z",
     "shell.execute_reply.started": "2022-05-18T13:52:05.817512Z"
    },
    "papermill": {
     "duration": 0.038681,
     "end_time": "2022-05-18T14:16:28.578256",
     "exception": false,
     "start_time": "2022-05-18T14:16:28.539575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_xy_depth(dic, device=device):\n",
    "    depth1 = dic[\"depth1\"]\n",
    "    pts1 = dic[\"points1\"].astype(int)\n",
    "    pts1_depth = torch.Tensor(depth1[pts1[:,1], pts1[:,0]])\n",
    "    pts1 = torch.Tensor(pts1)\n",
    "    \n",
    "    depth2 = dic[\"depth2\"]\n",
    "    pts2 = dic[\"points2\"].astype(int)\n",
    "    pts2_depth = torch.Tensor(depth2[pts2[:,1], pts2[:,0]])\n",
    "    pts2 = torch.Tensor(pts2)\n",
    "    \n",
    "    # discard points where depth == 0\n",
    "    valid_idxs = (pts1_depth > 0) & (pts2_depth > 0)\n",
    "    pts1_depth = pts1_depth[valid_idxs]\n",
    "    pts2_depth = pts2_depth[valid_idxs]\n",
    "    pts1_x = pts1[:, 0][valid_idxs]\n",
    "    pts1_y = pts1[:, 1][valid_idxs]\n",
    "    pts1 = torch.cat((pts1_x.unsqueeze(1), pts1_y.unsqueeze(1)), dim=1)\n",
    "    pts2_x = pts2[:, 0][valid_idxs]\n",
    "    pts2_y = pts2[:, 1][valid_idxs]\n",
    "    pts2 = torch.cat((pts2_x.unsqueeze(1), pts2_y.unsqueeze(1)), dim=1)\n",
    "    \n",
    "    xy_depth1 = torch.cat((pts1, pts1_depth.unsqueeze(1)), dim=1).unsqueeze(0)\n",
    "    xy_depth2 = torch.cat((pts2, pts2_depth.unsqueeze(1)), dim=1).unsqueeze(0)\n",
    "    xy_depth = torch.cat((xy_depth1, xy_depth2), dim=0).to(device)\n",
    "    xy_depth.requires_grad = False\n",
    "    return xy_depth\n",
    "\n",
    "def compute_F_from_depth_points_dic(depth_points_dic, n_iter=500):\n",
    "    xy_depth = make_xy_depth(depth_points_dic)\n",
    "    \n",
    "    principal_point1 = depth_points_dic[\"principal_point1\"].unsqueeze(0)\n",
    "    principal_point2 = depth_points_dic[\"principal_point2\"].unsqueeze(0)\n",
    "    principal_points = torch.cat((principal_point1, principal_point2), dim=0).to(device)\n",
    "    principal_points.requires_grad = False\n",
    "    \n",
    "    image_size1 = depth_points_dic[\"image_size1\"].unsqueeze(0)\n",
    "    image_size2 = depth_points_dic[\"image_size2\"].unsqueeze(0)\n",
    "    image_sizes = torch.cat((image_size1, image_size2), dim=0).to(device)\n",
    "    image_sizes.requires_grad = False\n",
    "    \n",
    "    log_R_absolute, T_absolute, focal_length = init_values()\n",
    "    \n",
    "    optimized_cameras = optimization(log_R_absolute, T_absolute, focal_length, principal_points, image_sizes, xy_depth, n_iter=n_iter)\n",
    "    print()\n",
    "    return calc_F(optimized_cameras[0].cpu(), optimized_cameras[1].cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63807f22",
   "metadata": {
    "papermill": {
     "duration": 0.021539,
     "end_time": "2022-05-18T14:16:28.621984",
     "exception": false,
     "start_time": "2022-05-18T14:16:28.600445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ***Inference***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d650464b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:16:28.678607Z",
     "iopub.status.busy": "2022-05-18T14:16:28.677925Z",
     "iopub.status.idle": "2022-05-18T14:16:43.585119Z",
     "shell.execute_reply": "2022-05-18T14:16:43.584466Z",
     "shell.execute_reply.started": "2022-05-18T14:13:16.204298Z"
    },
    "papermill": {
     "duration": 14.941365,
     "end_time": "2022-05-18T14:16:43.585282",
     "exception": false,
     "start_time": "2022-05-18T14:16:28.643917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\n",
      "  return torch.floor_divide(self, other)\n",
      "../input/pytorch3ddependencies/pytorch3d_dependencies/pytorch3d/transforms/transform3d.py:800: UserWarning: R is not a valid rotation matrix\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "took 9.639521598815918sec\n",
      "\n",
      "took 2.7724316120147705sec\n",
      "\n",
      "took 2.4895660877227783sec\n"
     ]
    }
   ],
   "source": [
    "F_dict = {}\n",
    "\n",
    "depth_points_dict = {}\n",
    "\n",
    "import time\n",
    "for i, row in enumerate(test_samples):\n",
    "    sample_id, batch_id, image_1_id, image_2_id = row\n",
    "    # Load the images.\n",
    "    st = time.time()\n",
    "    image1_filepath = f'{src}/test_images/{batch_id}/{image_1_id}.png'\n",
    "    image2_filepath = f'{src}/test_images/{batch_id}/{image_2_id}.png'\n",
    "    image_1 = load_torch_image(image1_filepath).to(device)\n",
    "    image_2 = load_torch_image(image2_filepath).to(device)\n",
    "    \n",
    "    depth1 = estimate_depth(image1_filepath, depth_estimator, transform)\n",
    "    depth2 = estimate_depth(image2_filepath, depth_estimator, transform)\n",
    "    \n",
    "    #LoFTR\n",
    "    mkpts0, mkpts1, confidence = match(image1_filepath, image2_filepath, matcher, device)\n",
    "    max_n_points = 50\n",
    "    sorted_idxs = np.argsort(confidence)[::-1]\n",
    "    idx_to_use = sorted_idxs[:min(max_n_points, len(sorted_idxs))]\n",
    "    mkpts0 = mkpts0[idx_to_use]\n",
    "    mkpts1 = mkpts1[idx_to_use]\n",
    "    \n",
    "    \n",
    "#     #SuperGlue\n",
    "#     sg_mkpts0, sg_mkpts1 = superglue_match(f'{src}/test_images/{batch_id}/{image_1_id}.png', f'{src}/test_images/{batch_id}/{image_2_id}.png', superglue, device)\n",
    "    \n",
    "#     mkpts0 = np.vstack((loftr_mkpts0, sg_mkpts0))\n",
    "#     mkpts1 = np.vstack((loftr_mkpts1, sg_mkpts1))\n",
    "\n",
    "    img1_h, img1_w = image_1.shape[2], image_1.shape[3]\n",
    "    img2_h, img2_w = image_2.shape[2], image_2.shape[3]\n",
    "\n",
    "    depth_points_dic = {\n",
    "        \"depth1\": depth1,\n",
    "        \"depth2\": depth2,\n",
    "        \"image1_filepath\": image1_filepath,\n",
    "        \"image2_filepath\": image2_filepath,\n",
    "        \"principal_point1\": torch.Tensor((img1_w/2, img1_h/2)),\n",
    "        \"principal_point2\": torch.Tensor((img2_w/2, img2_h/2)),\n",
    "        \"image_size1\": torch.Tensor((img1_h, img1_w)),\n",
    "        \"image_size2\": torch.Tensor((img2_h, img2_w)),\n",
    "        \"points1\": mkpts0,\n",
    "        \"points2\": mkpts1,\n",
    "    }\n",
    "    try:\n",
    "        F_dict[sample_id] = compute_F_from_depth_points_dic(depth_points_dic, n_iter=500)\n",
    "    except:\n",
    "        F_dict[sample_id] = np.zeros((3, 3))\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"took {time.time() - st}sec\")\n",
    "    \n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('sample_id,fundamental_matrix\\n')\n",
    "    for sample_id, F in F_dict.items():\n",
    "        f.write(f'{sample_id},{FlattenMatrix(F)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7066218f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:16:43.644429Z",
     "iopub.status.busy": "2022-05-18T14:16:43.641250Z",
     "iopub.status.idle": "2022-05-18T14:16:43.647259Z",
     "shell.execute_reply": "2022-05-18T14:16:43.647683Z",
     "shell.execute_reply.started": "2022-05-18T13:30:17.837107Z"
    },
    "papermill": {
     "duration": 0.037098,
     "end_time": "2022-05-18T14:16:43.647820",
     "exception": false,
     "start_time": "2022-05-18T14:16:43.610722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 59, 171,  71,  55,  57,  45,  91,  68,  79,  78,  93,  54,  56,\n",
       "        60, 136,  70, 145,  90, 123,  31, 139, 116, 193, 202, 182, 151,\n",
       "        13, 102, 237,  98,  92, 200, 100,  58,  51,  30, 104, 101, 105,\n",
       "        53, 159,  17,  99,  50,  82,  89,  12,   5, 115, 188])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_max_points = 50\n",
    "confidence.shape\n",
    "sorted_idxs = np.argsort(confidence)[::-1]\n",
    "idx_to_use = sorted_idxs[:min(n_max_points, len(sorted_idxs))]\n",
    "idx_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9ac2336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:16:43.701780Z",
     "iopub.status.busy": "2022-05-18T14:16:43.700862Z",
     "iopub.status.idle": "2022-05-18T14:16:43.703301Z",
     "shell.execute_reply": "2022-05-18T14:16:43.702776Z",
     "shell.execute_reply.started": "2022-05-18T13:42:49.257492Z"
    },
    "papermill": {
     "duration": 0.031278,
     "end_time": "2022-05-18T14:16:43.703460",
     "exception": false,
     "start_time": "2022-05-18T14:16:43.672182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mkpts0.shape\n",
    "# confidence[idx_to_use]\n",
    "# mkpts0[idx_to_use].shape\n",
    "# a = np.ones(10)[:10]\n",
    "# a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bcfdc76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:16:43.757569Z",
     "iopub.status.busy": "2022-05-18T14:16:43.756765Z",
     "iopub.status.idle": "2022-05-18T14:16:43.759450Z",
     "shell.execute_reply": "2022-05-18T14:16:43.759018Z",
     "shell.execute_reply.started": "2022-05-18T13:18:04.540393Z"
    },
    "papermill": {
     "duration": 0.031474,
     "end_time": "2022-05-18T14:16:43.759625",
     "exception": false,
     "start_time": "2022-05-18T14:16:43.728151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(optimized_cameras[0].R)\n",
    "# print(optimized_cameras[0].T)\n",
    "# print(optimized_cameras[0].K)\n",
    "# print(optimized_cameras[0].focal_length)\n",
    "# print(optimized_cameras[0].principal_point)\n",
    "# print(optimized_cameras[0].image_size)\n",
    "# print()\n",
    "# print(optimized_cameras[1].R)\n",
    "# print(optimized_cameras[1].T)\n",
    "# print(optimized_cameras[1].K)\n",
    "# print(optimized_cameras[1].focal_length)\n",
    "# print(optimized_cameras[1].principal_point)\n",
    "# print(optimized_cameras[1].image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db8496cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:16:43.814022Z",
     "iopub.status.busy": "2022-05-18T14:16:43.812898Z",
     "iopub.status.idle": "2022-05-18T14:16:43.815279Z",
     "shell.execute_reply": "2022-05-18T14:16:43.815696Z",
     "shell.execute_reply.started": "2022-05-18T13:18:04.549749Z"
    },
    "papermill": {
     "duration": 0.031421,
     "end_time": "2022-05-18T14:16:43.815837",
     "exception": false,
     "start_time": "2022-05-18T14:16:43.784416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dir(optimized_cameras[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75936d3",
   "metadata": {
    "papermill": {
     "duration": 0.024999,
     "end_time": "2022-05-18T14:16:43.866621",
     "exception": false,
     "start_time": "2022-05-18T14:16:43.841622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6910ad",
   "metadata": {
    "papermill": {
     "duration": 0.024862,
     "end_time": "2022-05-18T14:16:43.916001",
     "exception": false,
     "start_time": "2022-05-18T14:16:43.891139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb425fa",
   "metadata": {
    "papermill": {
     "duration": 0.02668,
     "end_time": "2022-05-18T14:16:43.969451",
     "exception": false,
     "start_time": "2022-05-18T14:16:43.942771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 148.427861,
   "end_time": "2022-05-18T14:16:47.394203",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-18T14:14:18.966342",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
